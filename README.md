# ğŸ§  Animal01

**Animal01** is a biologically inspired neural simulation framework designed for agent-based learning and behavior modeling.  
It features a sparse neural architecture with explicitly defined perceptor, hidden, and motor neurons.

Work in progress ...

---

## ğŸ“ Project Structure

```plaintext
.
â”œâ”€â”€ world/
â”‚   â”œâ”€â”€ helper.py         # Constants for cell types and actions
â”‚   â”œâ”€â”€ print_world.py    # Pretty printing of the world state
â”‚   â””â”€â”€ grid_world.py     # Main environment logic
â”œâ”€â”€ animal/
â”‚   â”œâ”€â”€ custom_layers.py  # SparseLinear layer with fixed connectivity
â”‚   â””â”€â”€ brain.py          # BrainModel and Brain state manager
â””â”€â”€ README.md             # You're here!
```

---

## ğŸ§± World Module

### `helper.py`

Defines constants:
```python
# Cell types
EMPTY, WALL, FOOD, ANIMAL = 0, 1, 2, 3
# Agent actions
STAY, UP, DOWN, LEFT, RIGHT = 0, 1, 2, 3, 4
```

---

### `print_world.py`

Renders a single board using unicode characters for visual inspection.

```bash
(A) - animal
 o  - food
 X  - wall
```

Example:
```
 ------------------------------
| X           o              X |
| o    (A)                     |
|    X              X        X |
| X                            |
| X                    X     o |
|    o  o        o             |
|       o                 X    |
 ------------------------------
```

---

### `grid_world.py`

Core environment class:
- Manages multiple 2D grid worlds in parallel.
- Allows agents to move, interact, and receive rewards.
- Automatically respawns food over time.

Key functions:
- `reset()` â€” initializes new worlds.
- `step_animal(actions)` â€” applies agent actions and returns rewards.
- `step_environment()` â€” adds food if needed.

---

## ğŸ§  Brain Module

### `custom_layers.py`

Defines `SparseLinear`, a sparse connectivity layer:
- Each neuron connects to a random subset of `K` input neurons.
- Efficient and biologically inspired.

```python
SparseLinear(in_features=1000, out_features=500, K=20)
```

---

### `brain.py`

Two main classes:

#### `BrainModel`

A sparse neural network composed of:
- **Perceptor neurons** (receive observations)
- **Hidden + Motor neurons** (evolve over time)
- Q-value output via a linear readout

```python
model = BrainModel(n_neurons=1000, n_perceptors=50, n_motors=10, n_actions=5, K=20)
```

#### `Brain`

Manages the agentâ€™s internal state:
- Holds a full state vector (neuron activations)
- Supports `observe()` and `think_and_update()` operations

---

## ğŸ” Simulation Loop

Example (see `grid_world.py` main block):

```python
for step in range(10):
    actions = torch.randint(0, 5, (B,), device=device)
    rewards = env.step_animal(actions)
    env.step_environment()
    env.print_board(b=0)
```

---

## âœ… Features

- âœ… Batched environments (parallel simulations)
- âœ… Toroidal (wrap-around) grid behavior
- âœ… Sparse Q-learning-ready neural controller
- âœ… Modular code for extensibility
- âœ… Human-readable debug printing

---

## ğŸš§ Planned Extensions

- [ ] Observation model for visual field / local patch
- [ ] Learning rule for evolving brain weights
- [ ] Brain-Environment interaction logs
- [ ] RL training loop

---

## ğŸ“¦ Requirements

- Python 3.8+
- PyTorch

Install dependencies:
```bash
pip install torch
```

---

## ğŸ§ª Run Tests

Each module includes sanity checks under `__main__`:
```bash
python world/print_world.py
python world/grid_world.py
python animal/custom_layers.py
python animal/brain.py
```

---

## ğŸ§  Inspiration

This project is inspired by:
- Sparse evolutionary algorithms
- Biological brains (perceptors, motors, internal state)
- Reinforcement learning agents

---

## ğŸ“œ License

MIT License

---

## ğŸ¤ Contributions

Welcome! The project is intentionally minimal â€” let us discuss.
