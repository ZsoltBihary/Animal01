import torch
from torch import Tensor
from torch.utils.data import Dataset
from tensordict import TensorDict
from core.tensordict_helper import Schema, State


class ReplayBuffer(Dataset):
    """
    Circular buffer to store (states, actions, target_qs) for Q-learning.
    - states: TensorDict defined by state_schema
    - actions: Tensor (B,)
    - target_qs: Tensor (B,)
    """
    def __init__(self, capacity: int, state_schema: Schema, device='cpu'):
        """
        Args:
            capacity: int, max number of transitions
            state_schema: dict[str, tuple(torch.Size, torch.dtype)]
                Example: { "x": (torch.Size([C, K, K]), torch.float32) }
            device: device for storage
        """
        self.capacity = capacity
        self.device = device
        self.state_schema = state_schema

        # Pre-allocate TensorDict for states
        self.states = TensorDict({
            k: torch.empty((capacity, *shape), dtype=dtype, device=device)
            for k, (shape, dtype) in state_schema.items()
        }, batch_size=[capacity], device=device)

        self.actions = torch.empty((capacity,), dtype=torch.long, device=device)
        self.target_qs = torch.empty((capacity,), dtype=torch.float32, device=device)

        self.index = 0
        self.size = 0

    def append(self, states: State, actions: Tensor, target_qs: Tensor):
        """
        Append a batch of transitions.
        Args:
            states: TensorDict with batch_size B
            actions: Tensor (B,)
            target_qs: Tensor (B,)
        """
        B = states.batch_size[0]

        # Ensure correct device
        states = states.to(self.device)
        actions = actions.to(self.device)
        target_qs = target_qs.to(self.device)

        end = self.index + B
        if end <= self.capacity:
            self.states[self.index:end] = states
            self.actions[self.index:end] = actions
            self.target_qs[self.index:end] = target_qs
        else:
            first_part = self.capacity - self.index
            second_part = B - first_part

            self.states[self.index:] = states[:first_part]
            self.actions[self.index:] = actions[:first_part]
            self.target_qs[self.index:] = target_qs[:first_part]

            self.states[:second_part] = states[first_part:]
            self.actions[:second_part] = actions[first_part:]
            self.target_qs[:second_part] = target_qs[first_part:]

        self.index = (self.index + B) % self.capacity
        self.size = min(self.size + B, self.capacity)

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        actual_idx = (self.index - self.size + idx) % self.capacity
        return (
            self.states[actual_idx],  # TensorDict
            self.actions[actual_idx],
            self.target_qs[actual_idx]
        )

        # state = self.states[actual_idx]
        # return (
        #     state,  # TensorDict
        #     self.actions[actual_idx],
        #     self.target_qs[actual_idx]
        # )


# import torch
# from torch.utils.data import Dataset

#
# class ReplayBuffer(Dataset):
#     # Circular buffer to store (states, actions, target_qs) data for the Q-learning pipeline.
#     # Generated by rollout, used in training.
#     def __init__(self, capacity, state_shape, device='cpu', dtype=torch.float32):
#         self.capacity = capacity
#         self.device = device
#
#         self.states = torch.empty((capacity, *state_shape), dtype=dtype, device=device)
#         self.actions = torch.empty((capacity,), dtype=torch.long, device=device)
#         self.target_qs = torch.empty((capacity,), dtype=dtype, device=device)
#
#         self.index = 0
#         self.size = 0
#
#     def append(self, states: torch.Tensor, actions: torch.Tensor, target_qs: torch.Tensor):
#         """
#         Append a batch of transitions to the buffer.
#         Args:
#             states:     Tensor (B, C, K, K)
#             actions:    Tensor (B,)
#             target_qs:  Tensor (B,)
#         """
#         B = states.size(0)
#
#         # Make sure tensors are on the correct device
#         states = states.to(self.device)
#         actions = actions.to(self.device)
#         target_qs = target_qs.to(self.device)
#
#         end = self.index + B
#         if end <= self.capacity:
#             self.states[self.index:end] = states
#             self.actions[self.index:end] = actions
#             self.target_qs[self.index:end] = target_qs
#         else:
#             first_part = self.capacity - self.index
#             second_part = B - first_part
#
#             self.states[self.index:] = states[:first_part]
#             self.actions[self.index:] = actions[:first_part]
#             self.target_qs[self.index:] = target_qs[:first_part]
#
#             self.states[:second_part] = states[first_part:]
#             self.actions[:second_part] = actions[first_part:]
#             self.target_qs[:second_part] = target_qs[first_part:]
#
#         self.index = (self.index + B) % self.capacity
#         self.size = min(self.size + B, self.capacity)
#
#     def __len__(self):
#         return self.size
#
#     def __getitem__(self, idx):
#         actual_idx = (self.index - self.size + idx) % self.capacity
#         return (
#             self.states[actual_idx],
#             self.actions[actual_idx],
#             self.target_qs[actual_idx]
#         )
#
#
# if __name__ == "__main__":
#     torch.manual_seed(0)
#
#     B = 4
#     capacity = 10
#     state_shape = (2, 3, 3)  # e.g. (C, K, K)
#
#     buffer = ReplayBuffer(capacity=capacity, state_shape=state_shape)
#
#     # Insert two batches of 4 transitions
#     for batch_id in range(2):
#         states = torch.ones((B, *state_shape)) * batch_id
#         actions = torch.arange(B) + batch_id * 10
#         target_qs = torch.arange(B, dtype=torch.float32) + batch_id * 100
#         buffer.append(states, actions, target_qs)
#
#     print("\nBuffer contents after 2 appends:")
#     for i in range(len(buffer)):
#         s, a, tq = buffer[i]
#         print(f"i={i} | action={a.item():>3} | target_q={tq.item():>6.1f} | state[0,0,0]={s[0, 0, 0].item()}")
#
#     # Insert batch to overflow the buffer
#     states = torch.ones((B, *state_shape)) * 9
#     actions = torch.arange(B) + 99
#     target_qs = torch.ones(B) * 999
#     buffer.append(states, actions, target_qs)
#
#     print("\nBuffer contents after overflow:")
#     for i in range(len(buffer)):
#         s, a, tq = buffer[i]
#         print(f"i={i} | action={a.item():>3} | target_q={tq.item():>6.1f} | state[0,0,0]={s[0, 0, 0].item()}")
#
#     # Test DataLoader integration
#     print("\nIterating with DataLoader:")
#     loader = torch.utils.data.DataLoader(buffer, batch_size=2, shuffle=True)
#     for batch in loader:
#         s, a, tq = batch
#         print(f"  batch_actions: {a.tolist()}")
