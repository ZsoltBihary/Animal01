# Deep Q-learning algorithm as in DRL_Minh_Presentation
import torch
import torch.nn as nn
from torch import Tensor


class QvalueModel(nn.Module):
    # QvalueModel owns self.parameters, self.layer1, self.layer2, etc.
    # No population-batching here !!!
    # QvalueModel does not own agent's state, this is external here !!!

    def plan(self, agent_state: Tensor) -> tuple[Tensor, Tensor]: ...
    # Stateless function: agent_state -> new_agent_state, q_values  (used for roll-out)

    def forward(self, agent_state: Tensor) -> Tensor:
        # Stateless function: agent_state -> q_values  (used for training)
        _, q_values = self.plan(agent_state)
        return q_values


class Agent:
    state: Tensor  # population-batched: shape(B, ...)
    def receive_observation(self, observation: Tensor): ...
    # observation -> modifies self.state in-place

    def update_and_plan(self, q_model: QvalueModel) -> Tensor:
        # self.state -> q_values, also modifies self.state in-place
        new_state, q_values = q_model(self.state)
        self.state = new_state
        return q_values

    def act(self, q_values: Tensor) -> Tensor: ...
    # q_values -> chosen_action


class Environment:
    state: Tensor
    def emit_observation(self) -> Tensor: ...
    # self.state -> observation
    def resolve_action(self, action: Tensor) -> Tensor: ...
    # action -> modifies self.state  (agent's effect on environment)
    def step(self): ...
    # modifies self.state  (environment's autonomous dynamics)


num_episode = 10
num_t = 100
B = 100        # population-batch size
batch_index = torch.arange(B)
Q_online = QvalueModel()
Q_target = QvalueModel()
agent = Agent()
env = Environment()

for episode in range(num_episode):

    for t in range(num_t):
        # Environment formats observation
        observation = env.emit_observation()  # observation is population-batched
        # Agent encodes observation into its internal state
        agent.receive_observation(observation)
        # Agent uses Q_online() to estimate q_values, and updates its internal state
        q_values = agent.update_and_plan(Q_online)  # q_values is population-batched
        # Agent uses q_values to decide on action (typically epsilon-greedy)
        action = agent.act(q_values)  # action is population-batched
        # Environment changes its state based on the agent's action, and returns the reward
        reward = env.resolve_action(action)  # reward is population-batched

        # ===== We calculate the bootstrapped target_q value here =====
        # Based on the q_values generated by Q_online(), the best action is:
        best_action = q_values.argmax(dim=1)
        # We
        target_q_values = Q_target(agent.state)
        target_q = target_q_values[batch_index, best_action]

        # # save agent.state
        # # state =
        # # determine target_q
        # q_values = Q_online(agent.state)
        # best_action = q_values.argmax(dim=1)
        # target_q_values = Q_target(agent.state)
        # target_q = target_q_values[batch_index, best_action]
