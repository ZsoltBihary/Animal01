import torch
from torch import Tensor
from torch.utils.data import Dataset
from reptile_world.config import Config, Action, Observation, Encoded, BrainState, Reward


class SDQLReplayBuffer(Dataset):
    # Circular buffer to store (states, actions, target_qs) data for the SDQ-learning pipeline.
    # Generated by rollout, used in training.
    def __init__(self, conf: Config):

        self.conf = conf
        # === Consume configuration parameters ===
        self.A = conf.num_actions
        self.C = conf.obs_channels
        self.K = conf.obs_size
        self.L = conf.brain_state_layers
        self.S = conf.brain_state_channels

        self.device = conf.buffer_device
        self.capacity = int(conf.steps_per_episode * conf.batch_size * conf.buffer_reuse)
        print("Buffer capacity: ", self.capacity)

        self.encoded_inputs = torch.empty((self.capacity, self.C + self.A, self.K, self.K),  # (N, C+A, K, K)
                                          dtype=torch.float32, device=self.device)
        self.brain_states = torch.empty((self.capacity, self.L, self.S, self.K, self.K),  # (N, L, S, K, K)
                                        dtype=torch.float32, device=self.device)
        self.actions = torch.empty((self.capacity,),  # (N, )
                                   dtype=torch.long, device=self.device)
        self.target_qs = torch.empty((self.capacity,),  # (N, )
                                     dtype=torch.float32, device=self.device)
        self.target_rewards = torch.empty((self.capacity,),  # (N, )
                                          dtype=torch.float32, device=self.device)
        self.target_observations = torch.empty((self.capacity, self.K, self.K),  # (N, K, K)
                                               dtype=torch.long, device=self.device)

        self.index = 0
        self.size = 0

    def append(self, encoded: Encoded, brain_state: BrainState, action: Action,
               target_q: Tensor, target_rew: Reward, target_obs: Observation):
        """
        Append a batch of transitions to the buffer.
        Args:
            encoded:     Tensor (B, C+A, K, K)
            brain_state: Tensor (B, L, S, K, K)
            action:      Tensor (B,)
            target_q:    Tensor (B,)
            target_rew:  Tensor (B,)
            target_obs:  Tensor (B, K, K)
        """
        B = encoded.size(0)

        # Make sure tensors are on the correct device
        encoded = encoded.to(self.device)
        brain_state = brain_state.to(self.device)
        action = action.to(self.device)
        target_q = target_q.to(self.device)
        target_rew = target_rew.to(self.device)
        target_obs = target_obs.to(self.device)

        end = self.index + B
        if end <= self.capacity:
            self.encoded_inputs[self.index:end] = encoded
            self.brain_states[self.index:end] = brain_state
            self.actions[self.index:end] = action
            self.target_qs[self.index:end] = target_q
            self.target_rewards[self.index:end] = target_rew
            self.target_observations[self.index:end] = target_obs
        else:
            first_part = self.capacity - self.index
            second_part = B - first_part

            self.encoded_inputs[self.index:] = encoded[:first_part]
            self.brain_states[self.index:] = brain_state[:first_part]
            self.actions[self.index:] = action[:first_part]
            self.target_qs[self.index:] = target_q[:first_part]
            self.target_rewards[self.index:] = target_rew[:first_part]
            self.target_observations[self.index:] = target_obs[:first_part]

            self.encoded_inputs[:second_part] = encoded[first_part:]
            self.brain_states[:second_part] = brain_state[first_part:]
            self.actions[:second_part] = action[first_part:]
            self.target_qs[:second_part] = target_q[first_part:]
            self.target_rewards[:second_part] = target_rew[first_part:]
            self.target_observations[:second_part] = target_obs[first_part:]

        self.index = (self.index + B) % self.capacity
        self.size = min(self.size + B, self.capacity)

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        actual_idx = (self.index - self.size + idx) % self.capacity
        return (
            self.encoded_inputs[actual_idx],
            self.brain_states[actual_idx],
            self.actions[actual_idx],
            self.target_qs[actual_idx],
            self.target_rewards[actual_idx],
            self.target_observations[actual_idx]
        )


# if __name__ == "__main__":
#     torch.manual_seed(0)
#
#     B = 4
#     capacity = 10
#     state_shape = (2, 3, 3)  # e.g. (C, K, K)
#
#     buffer = SDQLReplayBuffer(capacity=capacity, state_shape=state_shape)
#
#     # Insert two batches of 4 transitions
#     for batch_id in range(2):
#         states = torch.ones((B, *state_shape)) * batch_id
#         actions = torch.arange(B) + batch_id * 10
#         target_qs = torch.arange(B, dtype=torch.float32) + batch_id * 100
#         buffer.append(states, actions, target_qs)
#
#     print("\nBuffer contents after 2 appends:")
#     for i in range(len(buffer)):
#         s, a, tq = buffer[i]
#         print(f"i={i} | action={a.item():>3} | target_q={tq.item():>6.1f} | state[0,0,0]={s[0, 0, 0].item()}")
#
#     # Insert batch to overflow the buffer
#     states = torch.ones((B, *state_shape)) * 9
#     actions = torch.arange(B) + 99
#     target_qs = torch.ones(B) * 999
#     buffer.append(states, actions, target_qs)
#
#     print("\nBuffer contents after overflow:")
#     for i in range(len(buffer)):
#         s, a, tq = buffer[i]
#         print(f"i={i} | action={a.item():>3} | target_q={tq.item():>6.1f} | state[0,0,0]={s[0, 0, 0].item()}")
#
#     # Test DataLoader integration
#     print("\nIterating with DataLoader:")
#     loader = torch.utils.data.DataLoader(buffer, batch_size=2, shuffle=True)
#     for batch in loader:
#         s, a, tq = batch
#         print(f"  batch_actions: {a.tolist()}")
